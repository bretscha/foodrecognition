{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpJEzDG6DK2Q"
   },
   "source": [
    "# Train a custom object detection model with TensorFlow Lite Model Maker\n",
    "\n",
    "In this colab notebook, you'll learn how to use the [TensorFlow Lite Model Maker](https://www.tensorflow.org/lite/guide/model_maker) to train a custom object detection model to detect Android figurines and how to put the model on a Raspberry Pi.\n",
    "\n",
    "The Model Maker library uses *transfer learning* to simplify the process of training a TensorFlow Lite model using a custom dataset. Retraining a TensorFlow Lite model with your own custom dataset reduces the amount of training data required and will shorten the training time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prQ86DdtD317"
   },
   "source": [
    "Import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "l4QQTXHHATDS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 13:16:42.330944: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-19 13:16:42.330963: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import zipfile\n",
    "            \n",
    "from pandas import *\n",
    "from tflite_model_maker.config import ExportFormat, QuantizationConfig\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import object_detector\n",
    "\n",
    "from tflite_support import metadata\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.ERROR)\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def rename_picture_fileformat_in_annotaion(voc_dataset_dir, annotation, file_format):\n",
    "    tree = ET.parse('{}/Annotations/{}.xml'.format(voc_dataset_dir,annotation))\n",
    "    root = tree.getroot()\n",
    "    filename_node = root.find(\"filename\")\n",
    "    filename_node.text = \"{}.{}\".format(annotation, file_format)\n",
    "    tree.write('{}/Annotations/{}.xml'.format(voc_dataset_dir,annotation))\n",
    "    \n",
    "    \n",
    "def add_pose_to_object_annotation(voc_dataset_dir,annotation):\n",
    "    tree = ET.parse('{}/Annotations/{}.xml'.format(voc_dataset_dir,annotation))\n",
    "    root = tree.getroot()\n",
    "    object_nodes = root.findall(\"object\")\n",
    "    for object_node in object_nodes:\n",
    "        ET.SubElement(object_node, 'pose').text = \"Unspecified\"\n",
    "    tree.write('{}/Annotations/{}.xml'.format(voc_dataset_dir,annotation))\n",
    "    \n",
    "def convert_all_floats_to_ints(voc_dataset_dir, annotation):\n",
    "    tree = ET.parse('{}/Annotations/{}.xml'.format(voc_dataset_dir,annotation))\n",
    "    root = tree.getroot()\n",
    "    bndbox_nodes = root.findall(\"./object/bndbox\")\n",
    "    for bndbox_node in bndbox_nodes:\n",
    "        for child in bndbox_node.getchildren():\n",
    "            child.text = str(int(float(child.text)))\n",
    "    tree.write('{}/Annotations/{}.xml'.format(voc_dataset_dir,annotation))\n",
    "    \n",
    "\n",
    "def order_dataset(destination_dir, voc_dataset_dir):\n",
    "\n",
    "    shutil.rmtree(destination_dir) if os.path.isdir(destination_dir) else None\n",
    "    shutil.rmtree(voc_dataset_dir) if os.path.isdir(voc_dataset_dir) else None\n",
    "    \n",
    "    assert os.path.isfile(\"{}.zip\".format(voc_dataset_dir)), \"There is no dataset named \\\"{}.zip\\\"\".format(voc_dataset_dir) \n",
    "    with zipfile.ZipFile(\"{}.zip\".format(voc_dataset_dir),\"r\") as zip_dataset:\n",
    "        zip_dataset.extractall(voc_dataset_dir)\n",
    "\n",
    "    assert os.path.isdir(voc_dataset_dir), \"The directory to the PVOC dataset \\\"{}\\\" doesnt exist\".format(voc_dataset_dir) \n",
    "\n",
    "    os.makedirs(destination_dir, exist_ok=True)\n",
    "    with open(\"{}/ImageSets/Main/default.txt\".format(voc_dataset_dir)) as file:\n",
    "        lines = [line.rstrip() for line in file]\n",
    "        try:\n",
    "            for line in lines:\n",
    "                # Files might be in PNG File after exporting in CVAT but we need JPEG\n",
    "                if os.path.isfile('{}/JPEGImages/{}.PNG'.format(voc_dataset_dir,line)):\n",
    "                    im1 = Image.open('{}/JPEGImages/{}.PNG'.format(voc_dataset_dir, line))\n",
    "                    im1.save('{}/{}.jpeg'.format(destination_dir, line))\n",
    "                    rename_picture_fileformat_in_annotaion(voc_dataset_dir, line,\"jpeg\")\n",
    "                # IF files are in JPEG just move them\n",
    "                elif os.path.isfile('{}/JPEGImages/{}.jpeg'.format(voc_dataset_dir,line)):\n",
    "                    os.replace('{}/JPEGImages/{}.jpeg'.format(voc_dataset_dir, line),'{}/{}.jpeg'.format(destination_dir, line))\n",
    "                add_pose_to_object_annotation(voc_dataset_dir, line)\n",
    "                convert_all_floats_to_ints(voc_dataset_dir,line)\n",
    "                # Move the Annotation file to the destination_dir\n",
    "                os.replace('{}/Annotations/{}.xml'.format(voc_dataset_dir, line),'{}/{}.xml'.format(destination_dir, line))\n",
    "\n",
    "            # Move the labelmap file to the destination_dir\n",
    "            os.replace('{}/labelmap.txt'.format(voc_dataset_dir),'{}/labelmap.txt'.format(destination_dir))\n",
    "            shutil.rmtree(voc_dataset_dir)\n",
    "        except:\n",
    "            print(\"No files found in {}\".format(voc_dataset_dir))\n",
    "\n",
    "def get_labels() -> list:\n",
    "    data = read_csv(\"res/train/labelmap.txt\",sep=\":\",)\n",
    "    labels = data['# label'].tolist()\n",
    "    try:\n",
    "        labels.remove('background')\n",
    "    except:\n",
    "        print(\"no background variables in labelmap.txt\")\n",
    "    return labels\n",
    "\n",
    "def get_label_color(label, file=\"res/train/labelmap.txt\") -> list:\n",
    "    data = read_csv(file ,sep=\":\",)\n",
    "    index = data.index[data['# label'] == label].to_list()[0]\n",
    "    color_list = data['color_rgb'][index].replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")\n",
    "    color=()\n",
    "    for rgbstring in color_list:\n",
    "        color = color + (int(rgbstring),)\n",
    "    return color\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yxh3KInCFeB-"
   },
   "source": [
    "## Train the object detection model\n",
    "\n",
    "### Step 1: Load the dataset\n",
    "\n",
    "* Images in `res/train` are used to train the custom object detection model.\n",
    "* Images in `res/val` are used to check if the model can generalize well to new images that it hasn't seen before.\n",
    "If there is no train and val dirs, export your datasets from the CVAT server as described in the `README.md` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"res/train\"\n",
    "val_dir = \"res/val\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order the dataset into train and validation\n",
    "make sure to have exported your dataset with a 'PASCAL VOC' format in CVAT.  \n",
    "\n",
    "\n",
    "| IMPORTANT!!!  The validation and the train datasets HAVE to be exported SEPARATELY!  |\n",
    "|-----------------------------------------|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13912/968817525.py:44: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
      "  for child in bndbox_node.getchildren():\n"
     ]
    }
   ],
   "source": [
    "order_dataset(\"res/train\",\"res/train_food_pvoc\")\n",
    "order_dataset(\"res/val\",\"res/val_food_pvoc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "WiAahdsQAdT7"
   },
   "outputs": [],
   "source": [
    "train_data = object_detector.DataLoader.from_pascal_voc(\n",
    "    'res/train',\n",
    "    'res/train',\n",
    "    get_labels()\n",
    ")\n",
    "\n",
    "val_data = object_detector.DataLoader.from_pascal_voc(\n",
    "    'res/val',\n",
    "    'res/val',\n",
    "    get_labels()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNRhB8N7GHXj"
   },
   "source": [
    "### Step 2: Select a model architecture\n",
    "\n",
    "EfficientDet-Lite[0-4] are a family of mobile/IoT-friendly object detection models derived from the [EfficientDet](https://arxiv.org/abs/1911.09070) architecture.\n",
    "\n",
    "Here is the performance of each EfficientDet-Lite models compared to each others.\n",
    "\n",
    "| Model architecture | Size(MB)* | Latency(ms)** | Average Precision*** |\n",
    "|--------------------|-----------|---------------|----------------------|\n",
    "| EfficientDet-Lite0 | 4.4       | 146           | 25.69%               |\n",
    "| EfficientDet-Lite1 | 5.8       | 259           | 30.55%               |\n",
    "| EfficientDet-Lite2 | 7.2       | 396           | 33.97%               |\n",
    "| EfficientDet-Lite3 | 11.4      | 716           | 37.70%               |\n",
    "| EfficientDet-Lite4 | 19.9      | 1886          | 41.96%               |\n",
    "\n",
    "<i> * Size of the integer quantized models. <br/>\n",
    "** Latency measured on Raspberry Pi 4 using 4 threads on CPU. <br/>\n",
    "*** Average Precision is the mAP (mean Average Precision) on the COCO 2017 validation dataset.\n",
    "</i>\n",
    "\n",
    "In this notebook, we use EfficientDet-Lite0 to train our model. You can choose other model architectures depending on whether speed or accuracy is more important to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "GZOojrDHAY1J"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6a2e8331dd4fd2a898dd62440efbe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model', index=4, options=('efficientdet_lite0', 'efficientdet_lite1', 'efficientdet_lite…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "model_selector = widgets.Dropdown(\n",
    "    options=['efficientdet_lite0', 'efficientdet_lite1', 'efficientdet_lite2', 'efficientdet_lite3', 'efficientdet_lite4'],\n",
    "    value= 'efficientdet_lite4',\n",
    "    description='Model',\n",
    "    disabled=False,\n",
    ")\n",
    "display(model_selector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 13:21:22.456658: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-05-19 13:21:22.459723: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-19 13:21:22.459795: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-05-19 13:21:22.459842: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-05-19 13:21:22.459885: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-05-19 13:21:22.459930: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-05-19 13:21:22.459972: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-05-19 13:21:22.460014: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-05-19 13:21:22.460057: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-05-19 13:21:22.460065: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-05-19 13:21:22.471389: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "spec = model_spec.get(model_selector.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aeDU4mIM4ft"
   },
   "source": [
    "### Step 3: Train the TensorFlow model with the training data.\n",
    "\n",
    "* Set `epochs = 20`, which means it will go through the training dataset 20 times. You can look at the validation accuracy during training and stop when you see validation loss (`val_loss`) stop decreasing to avoid overfitting.\n",
    "* Set `batch_size = 4` here so you will see that it takes 15 steps to go through the 62 images in the training dataset.\n",
    "* Set `train_whole_model=True` to fine-tune the whole model instead of just training the head layer to improve accuracy. The trade-off is that it may take longer to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_MClfpsJAfda",
    "outputId": "cbfc9772-a3c7-4f60-a7af-a6032de27d2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 13:21:56.439659: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - ETA: 0s - det_loss: 1.2618 - cls_loss: 0.6870 - box_loss: 0.0115 - reg_l2_loss: 0.0929 - loss: 1.3547 - learning_rate: 0.0065 - gradient_norm: 3.0747"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 13:29:22.772340: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 474s 4s/step - det_loss: 1.2539 - cls_loss: 0.6831 - box_loss: 0.0114 - reg_l2_loss: 0.0929 - loss: 1.3468 - learning_rate: 0.0065 - gradient_norm: 3.0576 - val_det_loss: 1.4167 - val_cls_loss: 0.8516 - val_box_loss: 0.0113 - val_reg_l2_loss: 0.0930 - val_loss: 1.5097\n",
      "Epoch 2/20\n",
      "97/97 [==============================] - ETA: 0s - det_loss: 0.5972 - cls_loss: 0.3608 - box_loss: 0.0047 - reg_l2_loss: 0.0930 - loss: 0.6902 - learning_rate: 0.0049 - gradient_norm: 2.3599"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 13:36:14.864796: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 408s 4s/step - det_loss: 0.5971 - cls_loss: 0.3609 - box_loss: 0.0047 - reg_l2_loss: 0.0930 - loss: 0.6901 - learning_rate: 0.0049 - gradient_norm: 2.3722 - val_det_loss: 1.3181 - val_cls_loss: 0.8214 - val_box_loss: 0.0099 - val_reg_l2_loss: 0.0930 - val_loss: 1.4111\n",
      "Epoch 3/20\n",
      "97/97 [==============================] - ETA: 0s - det_loss: 0.4880 - cls_loss: 0.3055 - box_loss: 0.0037 - reg_l2_loss: 0.0930 - loss: 0.5810 - learning_rate: 0.0048 - gradient_norm: 2.0816"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 13:43:00.862400: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 407s 4s/step - det_loss: 0.4873 - cls_loss: 0.3050 - box_loss: 0.0036 - reg_l2_loss: 0.0930 - loss: 0.5803 - learning_rate: 0.0048 - gradient_norm: 2.0840 - val_det_loss: 1.2640 - val_cls_loss: 0.8175 - val_box_loss: 0.0089 - val_reg_l2_loss: 0.0930 - val_loss: 1.3569\n",
      "Epoch 4/20\n",
      "97/97 [==============================] - ETA: 0s - det_loss: 0.4302 - cls_loss: 0.2633 - box_loss: 0.0033 - reg_l2_loss: 0.0930 - loss: 0.5232 - learning_rate: 0.0046 - gradient_norm: 2.0040"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 13:49:46.528719: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 406s 4s/step - det_loss: 0.4300 - cls_loss: 0.2631 - box_loss: 0.0033 - reg_l2_loss: 0.0930 - loss: 0.5230 - learning_rate: 0.0046 - gradient_norm: 2.0061 - val_det_loss: 1.2675 - val_cls_loss: 0.8419 - val_box_loss: 0.0085 - val_reg_l2_loss: 0.0930 - val_loss: 1.3605\n",
      "Epoch 5/20\n",
      "97/97 [==============================] - ETA: 0s - det_loss: 0.4098 - cls_loss: 0.2454 - box_loss: 0.0033 - reg_l2_loss: 0.0930 - loss: 0.5028 - learning_rate: 0.0043 - gradient_norm: 2.0895"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 13:56:31.613993: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 434s 4s/step - det_loss: 0.4094 - cls_loss: 0.2451 - box_loss: 0.0033 - reg_l2_loss: 0.0930 - loss: 0.5024 - learning_rate: 0.0043 - gradient_norm: 2.0875 - val_det_loss: 1.2290 - val_cls_loss: 0.8109 - val_box_loss: 0.0084 - val_reg_l2_loss: 0.0930 - val_loss: 1.3219\n",
      "Epoch 6/20\n",
      "97/97 [==============================] - ETA: 0s - det_loss: 0.3826 - cls_loss: 0.2288 - box_loss: 0.0031 - reg_l2_loss: 0.0930 - loss: 0.4755 - learning_rate: 0.0040 - gradient_norm: 2.1084"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 14:03:46.215091: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 406s 4s/step - det_loss: 0.3829 - cls_loss: 0.2290 - box_loss: 0.0031 - reg_l2_loss: 0.0930 - loss: 0.4759 - learning_rate: 0.0040 - gradient_norm: 2.1102 - val_det_loss: 1.2310 - val_cls_loss: 0.8438 - val_box_loss: 0.0077 - val_reg_l2_loss: 0.0930 - val_loss: 1.3240\n",
      "Epoch 7/20\n",
      "97/97 [==============================] - ETA: 0s - det_loss: 0.3632 - cls_loss: 0.2172 - box_loss: 0.0029 - reg_l2_loss: 0.0930 - loss: 0.4562 - learning_rate: 0.0037 - gradient_norm: 2.2364"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 14:10:32.209195: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 405s 4s/step - det_loss: 0.3626 - cls_loss: 0.2173 - box_loss: 0.0029 - reg_l2_loss: 0.0930 - loss: 0.4556 - learning_rate: 0.0037 - gradient_norm: 2.2304 - val_det_loss: 1.2272 - val_cls_loss: 0.8487 - val_box_loss: 0.0076 - val_reg_l2_loss: 0.0930 - val_loss: 1.3202\n",
      "Epoch 8/20\n",
      "97/97 [==============================] - ETA: 0s - det_loss: 0.3255 - cls_loss: 0.2012 - box_loss: 0.0025 - reg_l2_loss: 0.0930 - loss: 0.4185 - learning_rate: 0.0033 - gradient_norm: 1.9587"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 14:17:17.231062: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 405s 4s/step - det_loss: 0.3273 - cls_loss: 0.2016 - box_loss: 0.0025 - reg_l2_loss: 0.0930 - loss: 0.4203 - learning_rate: 0.0033 - gradient_norm: 1.9834 - val_det_loss: 1.1524 - val_cls_loss: 0.7866 - val_box_loss: 0.0073 - val_reg_l2_loss: 0.0930 - val_loss: 1.2454\n",
      "Epoch 9/20\n",
      "97/97 [==============================] - ETA: 0s - det_loss: 0.3299 - cls_loss: 0.2036 - box_loss: 0.0025 - reg_l2_loss: 0.0930 - loss: 0.4228 - learning_rate: 0.0029 - gradient_norm: 2.2550"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 14:24:00.156296: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 404s 4s/step - det_loss: 0.3295 - cls_loss: 0.2036 - box_loss: 0.0025 - reg_l2_loss: 0.0930 - loss: 0.4225 - learning_rate: 0.0029 - gradient_norm: 2.2580 - val_det_loss: 1.1459 - val_cls_loss: 0.7958 - val_box_loss: 0.0070 - val_reg_l2_loss: 0.0930 - val_loss: 1.2389\n",
      "Epoch 10/20\n",
      "97/97 [==============================] - ETA: 0s - det_loss: 0.3057 - cls_loss: 0.1925 - box_loss: 0.0023 - reg_l2_loss: 0.0929 - loss: 0.3986 - learning_rate: 0.0025 - gradient_norm: 2.0832"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 14:30:43.425989: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 427s 4s/step - det_loss: 0.3062 - cls_loss: 0.1928 - box_loss: 0.0023 - reg_l2_loss: 0.0929 - loss: 0.3991 - learning_rate: 0.0025 - gradient_norm: 2.0822 - val_det_loss: 1.1236 - val_cls_loss: 0.7711 - val_box_loss: 0.0070 - val_reg_l2_loss: 0.0929 - val_loss: 1.2165\n",
      "Epoch 11/20\n",
      "97/97 [==============================] - ETA: 0s - det_loss: 0.2896 - cls_loss: 0.1874 - box_loss: 0.0020 - reg_l2_loss: 0.0929 - loss: 0.3825 - learning_rate: 0.0021 - gradient_norm: 1.8894"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 14:37:51.829256: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 405s 4s/step - det_loss: 0.2901 - cls_loss: 0.1876 - box_loss: 0.0021 - reg_l2_loss: 0.0929 - loss: 0.3831 - learning_rate: 0.0021 - gradient_norm: 1.8902 - val_det_loss: 1.1401 - val_cls_loss: 0.7980 - val_box_loss: 0.0068 - val_reg_l2_loss: 0.0929 - val_loss: 1.2330\n",
      "Epoch 12/20\n",
      "97/97 [==============================] - ETA: 0s - det_loss: 0.2903 - cls_loss: 0.1866 - box_loss: 0.0021 - reg_l2_loss: 0.0929 - loss: 0.3832 - learning_rate: 0.0017 - gradient_norm: 1.9418"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 14:44:37.216902: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 405s 4s/step - det_loss: 0.2895 - cls_loss: 0.1863 - box_loss: 0.0021 - reg_l2_loss: 0.0929 - loss: 0.3825 - learning_rate: 0.0017 - gradient_norm: 1.9386 - val_det_loss: 1.0671 - val_cls_loss: 0.7511 - val_box_loss: 0.0063 - val_reg_l2_loss: 0.0929 - val_loss: 1.1601\n",
      "Epoch 13/20\n",
      "97/97 [==============================] - ETA: 0s - det_loss: 0.2823 - cls_loss: 0.1833 - box_loss: 0.0020 - reg_l2_loss: 0.0929 - loss: 0.3753 - learning_rate: 0.0013 - gradient_norm: 1.8448"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 14:51:20.274175: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 402s 4s/step - det_loss: 0.2835 - cls_loss: 0.1836 - box_loss: 0.0020 - reg_l2_loss: 0.0929 - loss: 0.3765 - learning_rate: 0.0013 - gradient_norm: 1.8556 - val_det_loss: 1.0990 - val_cls_loss: 0.7740 - val_box_loss: 0.0065 - val_reg_l2_loss: 0.0929 - val_loss: 1.1920\n",
      "Epoch 14/20\n",
      "97/97 [==============================] - ETA: 0s - det_loss: 0.2698 - cls_loss: 0.1789 - box_loss: 0.0018 - reg_l2_loss: 0.0929 - loss: 0.3627 - learning_rate: 9.6790e-04 - gradient_norm: 1.7527"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 14:58:03.797340: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 403s 4s/step - det_loss: 0.2698 - cls_loss: 0.1791 - box_loss: 0.0018 - reg_l2_loss: 0.0929 - loss: 0.3627 - learning_rate: 9.6628e-04 - gradient_norm: 1.7517 - val_det_loss: 1.0498 - val_cls_loss: 0.7376 - val_box_loss: 0.0062 - val_reg_l2_loss: 0.0929 - val_loss: 1.1427\n",
      "Epoch 15/20\n",
      "97/97 [==============================] - ETA: 0s - det_loss: 0.2705 - cls_loss: 0.1790 - box_loss: 0.0018 - reg_l2_loss: 0.0929 - loss: 0.3635 - learning_rate: 6.6423e-04 - gradient_norm: 1.7655"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 15:04:46.737838: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 429s 4s/step - det_loss: 0.2707 - cls_loss: 0.1793 - box_loss: 0.0018 - reg_l2_loss: 0.0929 - loss: 0.3637 - learning_rate: 6.6285e-04 - gradient_norm: 1.7665 - val_det_loss: 1.0857 - val_cls_loss: 0.7705 - val_box_loss: 0.0063 - val_reg_l2_loss: 0.0929 - val_loss: 1.1786\n",
      "Epoch 16/20\n",
      "97/97 [==============================] - ETA: 0s - det_loss: 0.2691 - cls_loss: 0.1779 - box_loss: 0.0018 - reg_l2_loss: 0.0929 - loss: 0.3620 - learning_rate: 4.1063e-04 - gradient_norm: 1.7397"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 15:11:55.915986: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 405s 4s/step - det_loss: 0.2687 - cls_loss: 0.1776 - box_loss: 0.0018 - reg_l2_loss: 0.0929 - loss: 0.3616 - learning_rate: 4.0954e-04 - gradient_norm: 1.7402 - val_det_loss: 1.0659 - val_cls_loss: 0.7540 - val_box_loss: 0.0062 - val_reg_l2_loss: 0.0929 - val_loss: 1.1588\n",
      "Epoch 17/20\n",
      "97/97 [==============================] - ETA: 0s - det_loss: 0.2634 - cls_loss: 0.1759 - box_loss: 0.0018 - reg_l2_loss: 0.0929 - loss: 0.3563 - learning_rate: 2.1403e-04 - gradient_norm: 1.7199"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 15:18:40.599994: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 403s 4s/step - det_loss: 0.2633 - cls_loss: 0.1758 - box_loss: 0.0017 - reg_l2_loss: 0.0929 - loss: 0.3562 - learning_rate: 2.1324e-04 - gradient_norm: 1.7180 - val_det_loss: 1.0599 - val_cls_loss: 0.7484 - val_box_loss: 0.0062 - val_reg_l2_loss: 0.0929 - val_loss: 1.1528\n",
      "Epoch 18/20\n",
      "97/97 [==============================] - ETA: 0s - det_loss: 0.2614 - cls_loss: 0.1751 - box_loss: 0.0017 - reg_l2_loss: 0.0929 - loss: 0.3543 - learning_rate: 7.9782e-05 - gradient_norm: 1.7572"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 15:25:23.646042: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 403s 4s/step - det_loss: 0.2616 - cls_loss: 0.1751 - box_loss: 0.0017 - reg_l2_loss: 0.0929 - loss: 0.3545 - learning_rate: 7.9323e-05 - gradient_norm: 1.7566 - val_det_loss: 1.0569 - val_cls_loss: 0.7455 - val_box_loss: 0.0062 - val_reg_l2_loss: 0.0929 - val_loss: 1.1498\n",
      "Epoch 19/20\n",
      "97/97 [==============================] - ETA: 0s - det_loss: 0.2617 - cls_loss: 0.1732 - box_loss: 0.0018 - reg_l2_loss: 0.0929 - loss: 0.3546 - learning_rate: 1.1552e-05 - gradient_norm: 1.7328"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 15:32:07.337724: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 405s 4s/step - det_loss: 0.2618 - cls_loss: 0.1732 - box_loss: 0.0018 - reg_l2_loss: 0.0929 - loss: 0.3547 - learning_rate: 1.1434e-05 - gradient_norm: 1.7336 - val_det_loss: 1.0571 - val_cls_loss: 0.7449 - val_box_loss: 0.0062 - val_reg_l2_loss: 0.0929 - val_loss: 1.1500\n",
      "Epoch 20/20\n",
      "97/97 [==============================] - ETA: 0s - det_loss: 0.2646 - cls_loss: 0.1772 - box_loss: 0.0017 - reg_l2_loss: 0.0929 - loss: 0.3575 - learning_rate: 1.1201e-05 - gradient_norm: 1.6968"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 15:38:52.007809: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97/97 [==============================] - 429s 4s/step - det_loss: 0.2645 - cls_loss: 0.1772 - box_loss: 0.0017 - reg_l2_loss: 0.0929 - loss: 0.3574 - learning_rate: 1.1427e-05 - gradient_norm: 1.6983 - val_det_loss: 1.0665 - val_cls_loss: 0.7531 - val_box_loss: 0.0063 - val_reg_l2_loss: 0.0929 - val_loss: 1.1594\n"
     ]
    }
   ],
   "source": [
    "model = object_detector.create(train_data, model_spec=spec, batch_size=4, train_whole_model=True, epochs=20, validation_data=val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KB4hKeerMmh4"
   },
   "source": [
    "### Step 4. Evaluate the model with the validation data.\n",
    "\n",
    "After training the object detection model using the images in the training dataset, use the 10 images in the validation dataset to evaluate how the model performs against new data it has never seen before.\n",
    "\n",
    "As the default batch size is 64, it will take 1 step to go through the 10 images in the validation dataset.\n",
    "\n",
    "The evaluation metrics are same as [COCO](https://cocodataset.org/#detection-eval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OUqEpcYwAg8L",
    "outputId": "b359dc94-4906-4b5b-e022-77030b7a122f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 15:40:04.121227: W tensorflow/core/framework/dataset.cc:768] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 33s 11s/step\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AP': 0.2599284,\n",
       " 'AP50': 0.43577805,\n",
       " 'AP75': 0.29934555,\n",
       " 'APs': -1.0,\n",
       " 'APm': 0.3226509,\n",
       " 'APl': 0.37094292,\n",
       " 'ARmax1': 0.13378906,\n",
       " 'ARmax10': 0.42578125,\n",
       " 'ARmax100': 0.46816406,\n",
       " 'ARs': -1.0,\n",
       " 'ARm': 0.51744527,\n",
       " 'ARl': 0.5229358,\n",
       " 'AP_/tomato': 0.48699248,\n",
       " 'AP_/cheese': -1.0,\n",
       " 'AP_/mozzarella': -1.0,\n",
       " 'AP_/grape': 0.03286434}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NARVYk9rGLIl"
   },
   "source": [
    "### Step 5: Export as a TensorFlow Lite model.\n",
    "\n",
    "Export the trained object detection model to the TensorFlow Lite format by specifying which folder you want to export the quantized model to. The default post-training quantization technique is [full integer quantization](https://www.tensorflow.org/lite/performance/post_training_integer_quant). This allows the TensorFlow Lite model to be smaller, run faster on Raspberry Pi CPU and also compatible with the Google Coral EdgeTPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "_u3eFxoBAiqE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 15:40:40.535933: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2022-05-19 15:41:18.781658: W tensorflow/core/common_runtime/graph_constructor.cc:803] Node 'resample_p7/PartitionedCall' has 1 outputs but the _output_shapes attribute specifies shapes for 3 outputs. Output shapes may be inaccurate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated count of arithmetic ops: 13.736 G  ops, equivalently 6.868 G  MACs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 15:41:42.031445: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2022-05-19 15:41:42.031484: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2022-05-19 15:41:42.032243: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpvr4psyej\n",
      "2022-05-19 15:41:42.387951: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2022-05-19 15:41:42.387989: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpvr4psyej\n",
      "2022-05-19 15:41:43.056706: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-05-19 15:41:46.456906: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpvr4psyej\n",
      "2022-05-19 15:41:48.176830: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 6144590 microseconds.\n",
      "2022-05-19 15:41:51.438912: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-05-19 15:41:54.836981: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1963] Estimated count of arithmetic ops: 13.736 G  ops, equivalently 6.868 G  MACs\n",
      "\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 0\n",
      "2022-05-19 15:57:20.411872: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1963] Estimated count of arithmetic ops: 13.736 G  ops, equivalently 6.868 G  MACs\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated count of arithmetic ops: 13.736 G  ops, equivalently 6.868 G  MACs\n"
     ]
    }
   ],
   "source": [
    "model.export(export_dir='.', tflite_filename='models/foodrecognition_{}.tflite'.format(model_selector.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZcBmEigOCO3"
   },
   "source": [
    "### Step 6:  Evaluate the TensorFlow Lite model.\n",
    "\n",
    "Several factors can affect the model accuracy when exporting to TFLite:\n",
    "* [Quantization](https://www.tensorflow.org/lite/performance/model_optimization) helps shrinking the model size by 4 times at the expense of some accuracy drop.\n",
    "* The original TensorFlow model uses per-class [non-max supression (NMS)](https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH) for post-processing, while the TFLite model uses global NMS that's much faster but less accurate.\n",
    "Keras outputs maximum 100 detections while tflite outputs maximum 25 detections.\n",
    "\n",
    "Therefore you'll have to evaluate the exported TFLite model and compare its accuracy with the original TensorFlow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jbl8z9_wBPlr",
    "outputId": "a0893077-28d8-4362-89f2-d5197222ac79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 439s 3s/step\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AP': 0.117795005,\n",
       " 'AP50': 0.3465374,\n",
       " 'AP75': 0.045881458,\n",
       " 'APs': -1.0,\n",
       " 'APm': 0.16351096,\n",
       " 'APl': 0.099332824,\n",
       " 'ARmax1': 0.08945312,\n",
       " 'ARmax10': 0.23671874,\n",
       " 'ARmax100': 0.24941406,\n",
       " 'ARs': -1.0,\n",
       " 'ARm': 0.33772323,\n",
       " 'ARl': 0.11376147,\n",
       " 'AP_/tomato': 0.22117493,\n",
       " 'AP_/cheese': -1.0,\n",
       " 'AP_/mozzarella': -1.0,\n",
       " 'AP_/grape': 0.0144150825}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_tflite('models/foodrecognition_{}.tflite'.format(model_selector.value), val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWP3fEPaGNvd"
   },
   "source": [
    "## Compile the model for EdgeTPU\n",
    "\n",
    "Finally, we'll compile the model using `edgetpu_compiler` so that the model can run on [Google Coral EdgeTPU](https://coral.ai/).\n",
    "\n",
    "We start with installing the EdgeTPU compiler on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kK6AN1xVAsCb",
    "outputId": "0a199dc3-a4df-4b4b-ef2b-b9d88b14cb53"
   },
   "outputs": [],
   "source": [
    "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
    "!echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install edgetpu-compiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIGSdzXkEzrj"
   },
   "source": [
    "**Note:** When training the model using a custom dataset, beware that if your dataset includes more than 20 classes, you'll probably have slower inference speeds compared to if you have fewer classes. This is due to an aspect of the EfficientDet architecture in which a certain layer cannot compile for the Edge TPU when it carries more than 20 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzF6u0FZTAjF"
   },
   "source": [
    "Before compiling the `.tflite` file for the Edge TPU, it's important to consider whether your model will fit into the Edge TPU memory. \n",
    "\n",
    "The Edge TPU has approximately 8 MB of SRAM for [caching model paramaters](https://coral.ai/docs/edgetpu/compiler/#parameter-data-caching), so any model close to or over 8 MB will not fit onto the Edge TPU memory. That means the inference times are longer, because some model parameters must be fetched from the host system memory.\n",
    "\n",
    "One way to elimiate the extra latency is to use [model pipelining](https://coral.ai/docs/edgetpu/pipeline/), which splits the model into segments that can run on separate Edge TPUs in series. This can significantly reduce the latency for big models.\n",
    "\n",
    "The following table provides recommendations for the number of Edge TPUs to use with each EfficientDet-Lite model.\n",
    "\n",
    "| Model architecture | Minimum TPUs | Recommended TPUs\n",
    "|--------------------|-------|-------|\n",
    "| EfficientDet-Lite0 | 1     | 1     |\n",
    "| EfficientDet-Lite1 | 1     | 1     |\n",
    "| EfficientDet-Lite2 | 1     | 2     |\n",
    "| EfficientDet-Lite3 | 2     | 2     |\n",
    "| EfficientDet-Lite4 | 2     | 3     |\n",
    "\n",
    "If you need extra Edge TPUs for your model, then update `NUMBER_OF_TPUS` here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JyptUjakAwzz",
    "outputId": "7a3dad3b-eeb4-45e1-fb68-f0436ac5a933"
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_TPUS = 1\n",
    "\n",
    "!edgetpu_compiler android.tflite --num_segments=$NUMBER_OF_TPUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJYXucYWTGqZ"
   },
   "source": [
    "Finally, we'll copy the metadata, including the label file, from the original TensorFlow Lite model to the EdgeTPU model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LY1WrgMJBFd"
   },
   "outputs": [],
   "source": [
    "populator_dst = metadata.MetadataPopulator.with_model_file('android_edgetpu.tflite')\n",
    "\n",
    "with open('android.tflite', 'rb') as f:\n",
    "  populator_dst.load_metadata_and_associated_files(f.read())\n",
    "\n",
    "populator_dst.populate()\n",
    "updated_model_buf = populator_dst.get_model_buffer()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "Model Maker Object Detection for Android Figurine",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv38",
   "language": "python",
   "name": "venv38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
